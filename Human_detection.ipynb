{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip   # downloading annotations for train and validation 2017 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -c http://images.cocodataset.org/annotations.zip  # downloading annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o annotations_trainval2017.zip  # unzipping annotations for train and validation 2017 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd   # checking the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()   # clearing previous session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import layers, optimizers\n",
    "#from keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import load_model\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = '/home/image_training/val2017'    # image file directory\n",
    "annotation_json_files = '/home/image_training/annotations'   # annotation file directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(annotation_json_files,\"person_keypoints_val2017.json\")) as f:\n",
    "    j=json.load(f)      # joining annotation directory with the person keypoint directory then loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bound(low, high, value):            # bounding value between low and high\n",
    "    return min(high, max(low, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(j[\"annotations\"])     # length of the image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dim = 256         # considering image dimension (256, 256, 3)\n",
    "images = np.zeros((length, new_dim, new_dim, 3))     # initialization of image dataset\n",
    "box = np.zeros((length, 4))      # initialization of bounding box dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating image array with corresponding bounding box array as dataset\n",
    "for i in range(len(j[\"annotations\"])):\n",
    "    annotation_index = i # in range [0:len(j[\"annotations\"]) )  # extracting annotation index\n",
    "    image_id=j[\"annotations\"][annotation_index][\"image_id\"]   # extracting image id\n",
    "    image_index=[ind for ind in range(len(j[\"images\"])) if j[\"images\"][ind][\"id\"]==image_id][0]   # extracting image index\n",
    "    im=cv2.imread(os.path.join(image_files,j[\"images\"][image_index][\"file_name\"]))  # extracting image\n",
    "    h = im.shape[0] # extracting height of the image\n",
    "    w = im.shape[1] # extracting width of the image\n",
    "    crop = min(h,w)  # determining minimum of height and width \n",
    "    imgc = im[0:crop, 0:crop]  # croping the image by minimum of height or width\n",
    "    imgr = cv2.resize(imgc, (new_dim,new_dim))  # resizing the image by (256,256) dimension\n",
    "    bbox=np.array(j[\"annotations\"][annotation_index][\"bbox\"],dtype=\"int\")  # extracting bounding box\n",
    "    bbox[2:4] = bbox[0:2] + bbox[2:4]  # creating top left and bottom right point\n",
    "    bboxr = bbox * (new_dim/crop)  #  resizing bounding box to (256,256) dimension\n",
    "    bboxr = [(e / 256) for e in bboxr] # normalization of bounding box\n",
    "    bboxrn = [bound(0, 1, b) for b in bboxr]   # bounding values\n",
    "    images[i,:,:,:] = imgr / 255   # normalization of image pixels\n",
    "    box[i,:] = bboxrn \n",
    "    #cv2.rectangle(im,bbox[0:2],bbox[0:2]+bbox[2:4],(0,255,0),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape   # verifying the image shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini = 32  # images in each batch\n",
    "batch = 343  #  number of batches\n",
    "epoches = 300  # number of epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  human detection model training\n",
    "tic = time.time()     #start time\n",
    "# training process\n",
    "for j in range(0, epoches):    # for loop for epoches\n",
    "    for i in range(0, batch):        # for loop for batches\n",
    "        start = i*mini    #   starting index for each batch \n",
    "        end = (i + 1)*mini   # ending index for each batch \n",
    "        images_batch = images[start:end,:,:,:]   # slicing images for each batch \n",
    "        box_batch = box[start:end,:]     # slicing bounding boxes for each batch \n",
    "        print(\"No of epochs running = \" + str(j + 1) + \" and No of batch running = \" + str(i + 1))\n",
    "    \n",
    "        if i == 0 and j == 0:           # # for 1st iteration initialization of the model\n",
    "            model_h = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(new_dim, new_dim, 3)),\n",
    "                tf.keras.layers.MaxPooling2D(2,2),\n",
    "                tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "                tf.keras.layers.MaxPooling2D(2,2), \n",
    "                tf.keras.layers.Conv2D(64, (3,3), activation='relu'), \n",
    "                tf.keras.layers.MaxPooling2D(2,2),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(256, activation='relu'), \n",
    "                tf.keras.layers.Dense(128, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(32, activation='relu'),\n",
    "                tf.keras.layers.Dense(4, activation='sigmoid') \n",
    "                ])\n",
    "            model_h.summary()        # model structure\n",
    "            model_h.compile(                   # setting optimizer, loss function and metrics\n",
    "                            optimizer='sgd',\n",
    "                            loss='mse',\n",
    "                            metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "            history = model_h.fit(x = images_batch,     #   running the model for 1st iteration\n",
    "                                  y = box_batch,\n",
    "                                  steps_per_epoch=1,\n",
    "                                  epochs=1,\n",
    "                                  verbose=1)\n",
    "        \n",
    "            model_h.save('human_detection_model')                  #  saving the model\n",
    "            model_h.save_weights('human_detection_model_weights.h5')     #   saving the weights\n",
    "        \n",
    "        else:\n",
    "            model_h = load_model('human_detection_model')              #   loading the model\n",
    "            model_h.load_weights('human_detection_model_weights.h5')    #  loading the weights\n",
    "            history = model_h.fit(  x = images_batch,        #  running the model for rest of the images\n",
    "                                  y = box_batch,\n",
    "                                  steps_per_epoch=1,\n",
    "                                  epochs=1,\n",
    "                                  verbose=1)\n",
    "            model_h.save('human_detection_model')                         #  saving the model\n",
    "            model_h.save_weights('human_detection_model_weights.h5')      # saving the weights\n",
    "        \n",
    "toc = time.time()  #  end time \n",
    "print(\" Time elapsed = \" + str((toc - tic)/60) + \"minutes\")   # time elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h = load_model('human_detection_model')        # loading the model\n",
    "model_h.load_weights('human_detection_model_weights.h5')    # loading the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training accuracy\n",
    "\n",
    "begin = 1000   # starting index for evaluating training accuracy\n",
    "final = 2000   # ending index for evaluating training accuracy\n",
    "num = 850      # index which is used for visualization of training accuracy\n",
    "preds = model_h.predict(images[begin:final])      # prediction for images starting from begin to final\n",
    "preds1 = preds[num]        # taking specific index for visualization\n",
    "print(preds1)\n",
    "preds1 = preds1*256              # converting prediction bounding box from normalized to original value\n",
    "box_pred = preds1.astype(int)    # converting the values to integer\n",
    "print(box_pred)\n",
    "box_true = box[begin + num] * 256      # converting true bounding box from normalized to original value\n",
    "box_true = box_true.astype(int)        # converting the values to integer\n",
    "print(box_true)\n",
    "imaging = images[begin + num] * 255    # converting images from normalized to original value\n",
    "iming = imaging.astype(np.uint8)       # converting the values to integer\n",
    "cv2.rectangle(iming,box_pred[0:2],box_pred[2:4],(0,0,255),2)  # showing the images along with predicted bounding box\n",
    "cv2.rectangle(iming,box_true[0:2],box_true[2:4],(0,255,0),2)  # showing the images along with true bounding box\n",
    "cv2.imshow(\"Output\", iming)\n",
    "while(True):\n",
    "    k = cv2.waitKey(33)\n",
    "    if k == -1:  # if no key was pressed, -1 is returned\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing accuracy\n",
    "\n",
    "begin = 100             # starting index for evaluating testing accuracy\n",
    "final = 200             # ending index for evaluating testing accuracy\n",
    "num = 50                # index which is used for visualization of testing accuracy\n",
    "preds = model_h.predict(images[begin:final])   # prediction for images starting from begin to final\n",
    "preds1 = preds[num]    #   taking specific index for visualization\n",
    "print(preds1)\n",
    "preds1 = preds1*256     # converting prediction bounding box from normalized to original value\n",
    "box_pred = preds1.astype(int)   # converting the values to integer\n",
    "print(box_pred)\n",
    "print(box[begin + num] * 256)    \n",
    "imaging = images[begin + num] * 255   # converting images from normalized to original value\n",
    "iming = imaging.astype(np.uint8)      # converting the values to integer\n",
    "cv2.imshow(\"input\", iming)            # showing the images along with predicted bounding box\n",
    "while(True):\n",
    "    k = cv2.waitKey(33)\n",
    "    if k == -1:  # if no key was pressed, -1 is returned\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
